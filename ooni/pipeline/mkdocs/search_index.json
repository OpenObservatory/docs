{
    "docs": [
        {
            "location": "/", 
            "text": "Open Observatory Pipeline\n\n\nThis is the Open Observatory data processing pipeline\n\n\n\n\nSetup\n\n\nEdit \ninvoke.yaml\n based on \ninvoke.yaml.example\n to contain all the relevant\ntokens.\n\n\nInstall also all the python requirements in \nrequirements.txt\n.\n\n\nHow to run the pipeline tasks\n\n\nTo run on the AWS cloud do:\n\n\ninvoke start_computer\n\n\n\n\nIf you would like to run on the current machine the task that adds things to the\npostgres database you should run (after having installed the requirements in\n\nrequirements-computer.txt\n)\n\n\ninvoke add_headers_to_db\n\n\n\n\nTo re-run the data pipeline from scratch you shall first clean all the streams\nby running:\n\n\ninvoke clean_streams\n\n\n\n\nThen for every date you wish to import streams on you shall run:\n\n\ninvoke add_headers_to_db --workers=NUMBER_OF_CORES $YEAR\n\n\n\n\nWhen doing this AWS it's ideal to have 1 machine per year running.\n\n\nThis can be achieved by running the start_computer task like so:\n\n\ninvoke start_computer --invoke-command=\nadd_headers_to_db --workers=32 --halt --date-interval=2012\n\ninvoke start_computer --invoke-command=\nadd_headers_to_db --workers=32 --halt --date-interval=2013\n\ninvoke start_computer --invoke-command=\nadd_headers_to_db --workers=32 --halt --date-interval=2014\n\ninvoke start_computer --invoke-command=\nadd_headers_to_db --workers=32 --halt --date-interval=2015\n\n\n\n\n\nConfiguration\n\n\nBefore running the pipeline you should configure it by editing the\n\ninvoke.yaml\n file. An example configuration file is provided inside of\n\ninvoke.yaml.example\n.\n\n\nThe files you should probably be editing are the following:\n\n\ncore\n\n\n\n\n\n\ntmp_dir\n What directory should be used to store temporary files.\n\n\n\n\n\n\nssh_private_key_file\n What ssh private key shall be used by luigi for sshing into ssh:// machines.\n\n\n\n\n\n\nooni_pipeline_path\n The location on the ec2 instance where to look for the ooni-pipeline repository.\n\n\n\n\n\n\naws\n\n\n\n\n\n\naccess_key_id\n This is your AWS access key ID for spinning up EC2 instances.\n\n\n\n\n\n\nsecret_access_key\n This is your AWS secret token.\n\n\n\n\n\n\nssh_private_key_file\n This is a private key that will be used for sshing into the started machines.\n\n\n\n\n\n\npostgres\n\n\n\n\n\n\nhost\n The hostname of your postgres instance.\n\n\n\n\n\n\ndatabase\n The database name.\n\n\n\n\n\n\nusername\n The username to use when logging in.\n\n\n\n\n\n\npassword\n The password to use when logging in.\n\n\n\n\n\n\ntable\n The database table to use for writing report headers to.\n\n\n\n\n\n\nooni\n\n\n\n\nbridge_db_path\n A path to where you have a bridge_db.json file that\n    contains mappings between bridge IPs, their hashes and the ring they were\n    taken for (this is required for the sanitisation of bridge_reachability\n    reports).\n\n\n\n\nspark\n\n\n\n\n\n\nspark_submit\n Path to where the spark-submit command can be found.\n\n\n\n\n\n\nmaster\n The name of the yarn master node.\n\n\n\n\n\n\npapertrail\n\n\n\n\n\n\nhostname\n The hostname of the papertrail logging backend\n\n\n\n\n\n\nport\n The port of the papertrail logging backend\n\n\n\n\n\n\nkafka\n\n\nThis is currently not used\n\n\nspark\n\n\nThis is currently not used\n\n\nList of tasks\n\n\nTasks are run by using \npyinvoke\n and are defined inside\nof \ntasks.py\n.\n\n\nGenerate streams\n\n\nThis task can be run via:\n\n\ninvoke generate_streams --date-interval=DATE_INTERVAL [--src=URI]\n                        [--workers=NUM --dst-private=URI --dst-public=URI]\n                        [--halt]\n\n\n\n\nThe purpose of this task is to take the YAML reports that are located at the\naddress specified by the \nsrc\n URI and move them over into the private and\npublic bucket after having operated on them some transformations and\nsanitisations.\nThe transformations are in particular that of partitioning the data by data and\nconverting them to JSON. This means that all the reports from 2019-10-11 will\nend up in a JSON file named 2019-10-11.json.\n\n\nEach line of the JSON file will contain the full report header and an extra key\nused to identify if it's a header or a measurement.\n\n\nThe reason for splitting it into daily buckets is to avoid random seeking as\nmuch as possible.\n\n\n\n\ndate-interval\n\n\n\n\nThe date range that should be taken into consideration when running the\n\ngenerate_streams\n task. If no date range is specified it will run against all\nthe dates.\nThe format for the date range is that of the \nluigi DateInterval\nmodule\n.\nFor example: \n2019-10\n will be the full month of October 2019 or\n\n2019-10-29-2019-10-31\n will be the dates of the 29th and 30th of 2019.\n\n\n\n\nsrc\n default: \ns3n://ooni-private/reports-raw/yaml/\n\n\n\n\nWhere the reports should be read from. This is considered the \nmaster\ndataset\n of the pipeline.\n\n\n\n\nworkers\n default: 16\n\n\n\n\nThe number of CPU workers to use when running the operations.\n\n\n\n\ndst_private\n: default: \ns3n://ooni-private/\n\n\n\n\nThe target location in which to place the processed JSON streams. They will end\nup inside of \n$URI/reports-raw/streams/\n.\n\n\n\n\ndst_public\n: default: `s3n://ooni-public/\"\n\n\n\n\nThe target location in which to place the sanitised and processed JSON streams.\nThey will end up inside of \n$URI/reports-sanitised/streams/\n.\n\n\n\n\nhalt\n: default: disabled\n\n\n\n\nThis is an optional flag that indicates if we should or should not halt the\nmachine when done.\n\n\nUpload reports\n\n\nThis task can be run via:\n\n\ninvoke upload_reports --src=URI [--dst=URI --workers=INT --move --halt]\n\n\n\n\nThis task is responsible for moving or copying the reports from a certain\nincoming AWS bucket to the private ooni-pipeline bucket ready for being\nprocessed by the \ngenerate_streams\n task.\n\n\nIt will look inside of src for all files ending with \n.yaml\n.\n\n\nThe files will be renamed when moving them over to the dst directory using the\nfollowing format: \n{date}-{asn}-{test_name}-{df_version}-{ext}\n.\n\n\n\n\n\n\nsrc\n: From from where the reports should be copied or moved. In the\nooni-pipeline this is set to \ns3://ooni-incoming/\n that is the incoming\nbucket.\n\n\n\n\n\n\ndst\n default: \ns3n://ooni-private/reports-raw/yaml/\n\n\n\n\n\n\nWhere the reports should be moved or copied to.\n\n\n\n\nworkers\n default: 16\n\n\n\n\nThe number of CPU workers to use when running the operations.\n\n\n\n\nmove\n\n\n\n\nIf the source file should be deleted once it has been successfully copied.\n\n\n\n\nhalt\n: default: disabled\n\n\n\n\nThis is an optional flag that indicates if we should or should not halt the\nmachine when done.\n\n\nList reports\n\n\nThis task can be run via:\n\n\ninvoke list_reports --path=URI\n\n\n\n\nWill list all the files that appear to be OONI reports in a\ncertain directory.\n\n\n\n\npath\n: default: \ns3n://ooni-private/reports-raw/yaml/\n\n\n\n\nThat path to list reports inside of.\n\n\nClean streams\n\n\nThis task can be run via:\n\n\ninvoke clean_streams --dst-private=URI --dst-public=URI\n\n\n\n\nThis will delete all the files that are generated by the \ngenerate_streams\n\ntask.\nIn particular these files are:\n\n\n\n\n\n\nPRIVATE/reports-raw/streams\n\n\n\n\n\n\nPRIVATE/reports-sanitised/yaml\n\n\n\n\n\n\nPRIVATE/reports-sanitised/streams\n\n\n\n\n\n\nThe arguments are:\n\n\n\n\ndst_private\n: default: \ns3n://ooni-private/\n\n\n\n\nThe directory that contains the raw reports.\n\n\n\n\ndst-public\n: default: \ns3n://ooni-public/\n\n\n\n\nThe directory that contains the public reports.\n\n\nAdd headers to DB\n\n\nThis task can be run via:\n\n\ninvoke add_headers_to_db --date-interval=DATE_INTERVAL [--src=URI]\n                        [--workers=NUM --dst-private=URI --dst-public=URI]\n                        [--halt]\n\n\n\n\nWhen no date is specified it will run \nupload_reports\n on \ns3n://ooni-incoming\n\nby moving them, then run on these incoming reports the add_headers_to_db batch\noperation.\nWhen a date range is specified it will run the batch operation on such date.\n\n\nThe batch operation will sanitise the YAML reports generating their streams and\nthen add the report headers to the database.\nFor the schema of the database see the avro specification inside of\n\npipeline/helpers/report.py\n.\n\n\n\n\ndate-interval\n\n\n\n\nThe date range that should be taken into consideration when running the\n\ngenerate_streams\n task. If no date range is specified it will run against all\nthe dates.\nThe format for the date range is that of the \nluigi DateInterval\nmodule\n.\nFor example: \n2019-10\n will be the full month of October 2019 or\n\n2019-10-29-2019-10-31\n will be the dates of the 29th and 30th of 2019.\n\n\n\n\nsrc\n default: \ns3n://ooni-private/reports-raw/yaml/\n\n\n\n\nWhere the reports should be read from. This is considered the \nmaster\ndataset\n of the pipeline.\n\n\n\n\nworkers\n default: 16\n\n\n\n\nThe number of CPU workers to use when running the operations.\n\n\n\n\ndst_private\n: default: \ns3n://ooni-private/\n\n\n\n\nThe target location in which to place the processed JSON streams. They will end\nup inside of \n$URI/reports-raw/streams/\n.\n\n\n\n\ndst_public\n: default: `s3n://ooni-public/\"\n\n\n\n\nThe target location in which to place the sanitised and processed JSON streams.\nThey will end up inside of \n$URI/reports-sanitised/streams/\n.\n\n\n\n\nhalt\n: default: disabled\n\n\n\n\nThis is an optional flag that indicates if we should or should not halt the\nmachine when done.\n\n\nSync reports\n\n\nThis task can be run via:\n\n\ninvoke sync_reports --srcs=URI [--dst-private=URI --workers=INT --halt]\n\n\n\n\nThis task is responsible for moving the reports from a certain set of sources\nto the incoming bucket.\nThis task is usually run on collectors to place the data they have gathered\ninto the incoming AWS bucket.\n\n\nIt will look inside of src for all files ending with \n.yaml\n.\n\n\n\n\nsrcs\n: default: \nssh://root@bouncer.infra.ooni.nu/data/bouncer/archive\n\n\n\n\nThe source directories from where to look for OONI reports.\n\n\n\n\ndst-private\n default: \ns3n://ooni-incoming/\n\n\n\n\nWhere the reports should be moved to.\n\n\n\n\nworkers\n default: 16\n\n\n\n\nThe number of CPU workers to use when running the operations.\n\n\n\n\nhalt\n: default: disabled\n\n\n\n\nThis is an optional flag that indicates if we should or should not halt the\nmachine when done.\n\n\nStart computer\n\n\nThis task can be run via:\n\n\ninvoke start_computer [--private-key=PATH --instance-type=INSTANCE_TYPE ]\n                      [--invoke_command=INVOKE_COMMAND]\n\n\n\n\n\n\nprivate-key\n: default: \nprivate/ooni-pipeline.pem\n\n\n\n\nThe private key to be used to ssh into the machine.\n\n\n\n\ninstance_type\n: default: \nc3.8xlarge\n\n\n\n\nThe type of AWS EC2 instance to start. A full list of them can be found here:\n\nhttps://aws.amazon.com/ec2/instance-types/\n\n\n\n\ninvoke_command\n: default: \nadd_headers_to_db --workers=32 --halt\n\n\n\n\nThe invoke command to be run once the machine is started. Remember that it may\nbe important to also run the --halt to avoid extra costs.\n\n\nSpark apps\n\n\nThis task can be run via:\n\n\ninvoke [--private-key=PATH instance-type=INSTANCE_TYPE --invoke_command=INVOKE_COMMAND]\n\n\n\n\nThis task will run the batch spark based apps on a hadoop cluster. The current\nbatch operations are responsible for inspecting the sanitised streams bucketed\nby date located inside of \n--src\n, generating some database views based upon them\nand writing a processed JSON file inside of \n--dst\n to indicate that the certain\ndate has been processed.\n\n\n\n\ndate-interval\n\n\n\n\nThe date range that should be taken into consideration when running the\n\ngenerate_streams\n task. If no date range is specified it will run against all\nthe dates.\nThe format for the date range is that of the \nluigi DateInterval\nmodule\n.\nFor example: \n2019-10\n will be the full month of October 2019 or\n\n2019-10-29-2019-10-31\n will be the dates of the 29th and 30th of 2019.\n\n\n\n\nsrc\n: default: \ns3n://ooni-public/reports-sanitised/streams/\n\n\n\n\nFrom where to read the JSON streams from.\n\n\n\n\ndst\n: default: \ns3n://ooni-public/processed/\n\n\n\n\nWhere to write a file to indicate a certain date has been processed.\n\n\n\n\nworkers\n: default: 3\n\n\n\n\nThe number of CPU workers to use when running the operations.\n\n\nSpark submit\n\n\nThis task is work in progress and is not throughly tested, it's for running\nspark scripts on a hadoop cluster.", 
            "title": "Home"
        }, 
        {
            "location": "/#open-observatory-pipeline", 
            "text": "This is the Open Observatory data processing pipeline", 
            "title": "Open Observatory Pipeline"
        }, 
        {
            "location": "/#setup", 
            "text": "Edit  invoke.yaml  based on  invoke.yaml.example  to contain all the relevant\ntokens.  Install also all the python requirements in  requirements.txt .", 
            "title": "Setup"
        }, 
        {
            "location": "/#how-to-run-the-pipeline-tasks", 
            "text": "To run on the AWS cloud do:  invoke start_computer  If you would like to run on the current machine the task that adds things to the\npostgres database you should run (after having installed the requirements in requirements-computer.txt )  invoke add_headers_to_db  To re-run the data pipeline from scratch you shall first clean all the streams\nby running:  invoke clean_streams  Then for every date you wish to import streams on you shall run:  invoke add_headers_to_db --workers=NUMBER_OF_CORES $YEAR  When doing this AWS it's ideal to have 1 machine per year running.  This can be achieved by running the start_computer task like so:  invoke start_computer --invoke-command= add_headers_to_db --workers=32 --halt --date-interval=2012 \ninvoke start_computer --invoke-command= add_headers_to_db --workers=32 --halt --date-interval=2013 \ninvoke start_computer --invoke-command= add_headers_to_db --workers=32 --halt --date-interval=2014 \ninvoke start_computer --invoke-command= add_headers_to_db --workers=32 --halt --date-interval=2015", 
            "title": "How to run the pipeline tasks"
        }, 
        {
            "location": "/#configuration", 
            "text": "Before running the pipeline you should configure it by editing the invoke.yaml  file. An example configuration file is provided inside of invoke.yaml.example .  The files you should probably be editing are the following:  core    tmp_dir  What directory should be used to store temporary files.    ssh_private_key_file  What ssh private key shall be used by luigi for sshing into ssh:// machines.    ooni_pipeline_path  The location on the ec2 instance where to look for the ooni-pipeline repository.    aws    access_key_id  This is your AWS access key ID for spinning up EC2 instances.    secret_access_key  This is your AWS secret token.    ssh_private_key_file  This is a private key that will be used for sshing into the started machines.    postgres    host  The hostname of your postgres instance.    database  The database name.    username  The username to use when logging in.    password  The password to use when logging in.    table  The database table to use for writing report headers to.    ooni   bridge_db_path  A path to where you have a bridge_db.json file that\n    contains mappings between bridge IPs, their hashes and the ring they were\n    taken for (this is required for the sanitisation of bridge_reachability\n    reports).   spark    spark_submit  Path to where the spark-submit command can be found.    master  The name of the yarn master node.    papertrail    hostname  The hostname of the papertrail logging backend    port  The port of the papertrail logging backend    kafka  This is currently not used  spark  This is currently not used", 
            "title": "Configuration"
        }, 
        {
            "location": "/#list-of-tasks", 
            "text": "Tasks are run by using  pyinvoke  and are defined inside\nof  tasks.py .  Generate streams  This task can be run via:  invoke generate_streams --date-interval=DATE_INTERVAL [--src=URI]\n                        [--workers=NUM --dst-private=URI --dst-public=URI]\n                        [--halt]  The purpose of this task is to take the YAML reports that are located at the\naddress specified by the  src  URI and move them over into the private and\npublic bucket after having operated on them some transformations and\nsanitisations.\nThe transformations are in particular that of partitioning the data by data and\nconverting them to JSON. This means that all the reports from 2019-10-11 will\nend up in a JSON file named 2019-10-11.json.  Each line of the JSON file will contain the full report header and an extra key\nused to identify if it's a header or a measurement.  The reason for splitting it into daily buckets is to avoid random seeking as\nmuch as possible.   date-interval   The date range that should be taken into consideration when running the generate_streams  task. If no date range is specified it will run against all\nthe dates.\nThe format for the date range is that of the  luigi DateInterval\nmodule .\nFor example:  2019-10  will be the full month of October 2019 or 2019-10-29-2019-10-31  will be the dates of the 29th and 30th of 2019.   src  default:  s3n://ooni-private/reports-raw/yaml/   Where the reports should be read from. This is considered the  master\ndataset  of the pipeline.   workers  default: 16   The number of CPU workers to use when running the operations.   dst_private : default:  s3n://ooni-private/   The target location in which to place the processed JSON streams. They will end\nup inside of  $URI/reports-raw/streams/ .   dst_public : default: `s3n://ooni-public/\"   The target location in which to place the sanitised and processed JSON streams.\nThey will end up inside of  $URI/reports-sanitised/streams/ .   halt : default: disabled   This is an optional flag that indicates if we should or should not halt the\nmachine when done.  Upload reports  This task can be run via:  invoke upload_reports --src=URI [--dst=URI --workers=INT --move --halt]  This task is responsible for moving or copying the reports from a certain\nincoming AWS bucket to the private ooni-pipeline bucket ready for being\nprocessed by the  generate_streams  task.  It will look inside of src for all files ending with  .yaml .  The files will be renamed when moving them over to the dst directory using the\nfollowing format:  {date}-{asn}-{test_name}-{df_version}-{ext} .    src : From from where the reports should be copied or moved. In the\nooni-pipeline this is set to  s3://ooni-incoming/  that is the incoming\nbucket.    dst  default:  s3n://ooni-private/reports-raw/yaml/    Where the reports should be moved or copied to.   workers  default: 16   The number of CPU workers to use when running the operations.   move   If the source file should be deleted once it has been successfully copied.   halt : default: disabled   This is an optional flag that indicates if we should or should not halt the\nmachine when done.  List reports  This task can be run via:  invoke list_reports --path=URI  Will list all the files that appear to be OONI reports in a\ncertain directory.   path : default:  s3n://ooni-private/reports-raw/yaml/   That path to list reports inside of.  Clean streams  This task can be run via:  invoke clean_streams --dst-private=URI --dst-public=URI  This will delete all the files that are generated by the  generate_streams \ntask.\nIn particular these files are:    PRIVATE/reports-raw/streams    PRIVATE/reports-sanitised/yaml    PRIVATE/reports-sanitised/streams    The arguments are:   dst_private : default:  s3n://ooni-private/   The directory that contains the raw reports.   dst-public : default:  s3n://ooni-public/   The directory that contains the public reports.  Add headers to DB  This task can be run via:  invoke add_headers_to_db --date-interval=DATE_INTERVAL [--src=URI]\n                        [--workers=NUM --dst-private=URI --dst-public=URI]\n                        [--halt]  When no date is specified it will run  upload_reports  on  s3n://ooni-incoming \nby moving them, then run on these incoming reports the add_headers_to_db batch\noperation.\nWhen a date range is specified it will run the batch operation on such date.  The batch operation will sanitise the YAML reports generating their streams and\nthen add the report headers to the database.\nFor the schema of the database see the avro specification inside of pipeline/helpers/report.py .   date-interval   The date range that should be taken into consideration when running the generate_streams  task. If no date range is specified it will run against all\nthe dates.\nThe format for the date range is that of the  luigi DateInterval\nmodule .\nFor example:  2019-10  will be the full month of October 2019 or 2019-10-29-2019-10-31  will be the dates of the 29th and 30th of 2019.   src  default:  s3n://ooni-private/reports-raw/yaml/   Where the reports should be read from. This is considered the  master\ndataset  of the pipeline.   workers  default: 16   The number of CPU workers to use when running the operations.   dst_private : default:  s3n://ooni-private/   The target location in which to place the processed JSON streams. They will end\nup inside of  $URI/reports-raw/streams/ .   dst_public : default: `s3n://ooni-public/\"   The target location in which to place the sanitised and processed JSON streams.\nThey will end up inside of  $URI/reports-sanitised/streams/ .   halt : default: disabled   This is an optional flag that indicates if we should or should not halt the\nmachine when done.  Sync reports  This task can be run via:  invoke sync_reports --srcs=URI [--dst-private=URI --workers=INT --halt]  This task is responsible for moving the reports from a certain set of sources\nto the incoming bucket.\nThis task is usually run on collectors to place the data they have gathered\ninto the incoming AWS bucket.  It will look inside of src for all files ending with  .yaml .   srcs : default:  ssh://root@bouncer.infra.ooni.nu/data/bouncer/archive   The source directories from where to look for OONI reports.   dst-private  default:  s3n://ooni-incoming/   Where the reports should be moved to.   workers  default: 16   The number of CPU workers to use when running the operations.   halt : default: disabled   This is an optional flag that indicates if we should or should not halt the\nmachine when done.  Start computer  This task can be run via:  invoke start_computer [--private-key=PATH --instance-type=INSTANCE_TYPE ]\n                      [--invoke_command=INVOKE_COMMAND]   private-key : default:  private/ooni-pipeline.pem   The private key to be used to ssh into the machine.   instance_type : default:  c3.8xlarge   The type of AWS EC2 instance to start. A full list of them can be found here: https://aws.amazon.com/ec2/instance-types/   invoke_command : default:  add_headers_to_db --workers=32 --halt   The invoke command to be run once the machine is started. Remember that it may\nbe important to also run the --halt to avoid extra costs.  Spark apps  This task can be run via:  invoke [--private-key=PATH instance-type=INSTANCE_TYPE --invoke_command=INVOKE_COMMAND]  This task will run the batch spark based apps on a hadoop cluster. The current\nbatch operations are responsible for inspecting the sanitised streams bucketed\nby date located inside of  --src , generating some database views based upon them\nand writing a processed JSON file inside of  --dst  to indicate that the certain\ndate has been processed.   date-interval   The date range that should be taken into consideration when running the generate_streams  task. If no date range is specified it will run against all\nthe dates.\nThe format for the date range is that of the  luigi DateInterval\nmodule .\nFor example:  2019-10  will be the full month of October 2019 or 2019-10-29-2019-10-31  will be the dates of the 29th and 30th of 2019.   src : default:  s3n://ooni-public/reports-sanitised/streams/   From where to read the JSON streams from.   dst : default:  s3n://ooni-public/processed/   Where to write a file to indicate a certain date has been processed.   workers : default: 3   The number of CPU workers to use when running the operations.  Spark submit  This task is work in progress and is not throughly tested, it's for running\nspark scripts on a hadoop cluster.", 
            "title": "List of tasks"
        }
    ]
}