<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Open Observatory Pipeline</title>
  

  <link rel="shortcut icon" href="./img/favicon.ico">

  
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="./css/theme.css" type="text/css" />
  <link rel="stylesheet" href="./css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="./css/highlight.css">

  
  <script>
    // Current page data
    var mkdocs_page_name = "None";
  </script>
  
  <script src="./js/jquery-2.1.1.min.js"></script>
  <script src="./js/modernizr-2.8.3.min.js"></script>
  <script type="text/javascript" src="./js/highlight.pack.js"></script>
  <script src="./js/theme.js"></script> 

  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href="." class="icon icon-home"> Open Observatory Pipeline</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="./search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
        <ul class="current">
          
            <li>
    <li class="toctree-l1 current">
        <a class="current" href=".">Home</a>
        
            <ul>
            
                <li class="toctree-l3"><a href="#open-observatory-pipeline">Open Observatory Pipeline</a></li>
                
                    <li><a class="toctree-l4" href="#setup">Setup</a></li>
                
                    <li><a class="toctree-l4" href="#how-to-run-the-pipeline-tasks">How to run the pipeline tasks</a></li>
                
                    <li><a class="toctree-l4" href="#list-of-tasks">List of tasks</a></li>
                
                    <li><a class="toctree-l4" href="#more-sauce">More sauce</a></li>
                
            
            </ul>
        
    </li>
<li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href=".">Open Observatory Pipeline</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".">Docs</a> &raquo;</li>
    
      
    
    <li>Home</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="open-observatory-pipeline">Open Observatory Pipeline</h1>
<p>This is the Open Observatory data processing pipeline</p>
<p><img alt="ooni-pipeline-ng architecture diagram" src="https://raw.githubusercontent.com/TheTorProject/ooni-pipeline-ng/master/docs/ooni-pipeline-ng-architecture.png" /></p>
<h2 id="setup">Setup</h2>
<p>Edit <code>invoke.yaml</code> based on <code>invoke.yaml.example</code> to contain all the relevant
tokens.</p>
<p>Install also all the python requirements in <code>requirements.txt</code>.</p>
<h2 id="how-to-run-the-pipeline-tasks">How to run the pipeline tasks</h2>
<p>To run on the AWS cloud do:</p>
<pre><code>invoke start_computer
</code></pre>

<p>If you would like to run on the current machine the task that adds things to the
postgres database you should run (after having installed the requirements in
<code>requirements-computer.txt</code>)</p>
<pre><code>invoke add_headers_to_db
</code></pre>

<h2 id="list-of-tasks">List of tasks</h2>
<h3 id="generate-streams">Generate streams</h3>
<p>This task can be run via:</p>
<pre><code>invoke generate_streams --date-interval=DATE_INTERVAL [--src=URI]
                        [--workers=NUM --dst-private=URI --dst-public=URI]
                        [--halt]
</code></pre>

<p>The purpose of this task is to take the YAML reports that are located at the
address specified by the <code>src</code> URI and move them over into the private and
public bucket after having operated on them some transformations and
sanitisations.
The transformations are in particular that of partitioning the data by data and
converting them to JSON. This means that all the reports from 2019-10-11 will
end up in a JSON file named 2019-10-11.json.</p>
<p>Each line of the JSON file will contain the full report header and an extra key
used to identify if it's a header or a measurement.</p>
<p>The reason for splitting it into daily buckets is to avoid random seeking as
much as possible.</p>
<ul>
<li><strong>date-interval</strong></li>
</ul>
<p>The date range that should be taken into consideration when running the
<code>generate_streams</code> task. If no date range is specified it will run against all
the dates.
The format for the date range is that of the <a href="http://luigi.readthedocs.org/en/stable/api/luigi.date_interval.html">luigi DateInterval
module</a>.
For example: <code>2019-10</code> will be the full month of October 2019 or
<code>2019-10-29-2019-10-31</code> will be the dates of the 29th and 30th of 2019.</p>
<ul>
<li><strong>src</strong> default: <code>s3n://ooni-private/reports-raw/yaml/</code></li>
</ul>
<p>Where the reports should be read from. This is considered the <strong>master
dataset</strong> of the pipeline.</p>
<ul>
<li><strong>workers</strong> default: 16</li>
</ul>
<p>The number of CPU workers to use when running the operations.</p>
<ul>
<li><em>dst_private</em>: default: <code>s3n://ooni-private/</code></li>
</ul>
<p>The target location in which to place the processed JSON streams. They will end
up inside of <code>$URI/reports-raw/streams/</code>.</p>
<ul>
<li><strong>dst_public</strong>: default: `s3n://ooni-public/"</li>
</ul>
<p>The target location in which to place the sanitised and processed JSON streams.
They will end up inside of <code>$URI/reports-sanitised/streams/</code>.</p>
<ul>
<li><strong>halt</strong>: default: disabled</li>
</ul>
<p>This is an optional flag that indicates if we should or should not halt the
machine when done.</p>
<h3 id="upload-reports">Upload reports</h3>
<p>This task can be run via:</p>
<pre><code>invoke upload_reports --src=URI [--dst=URI --workers=INT --move --halt]
</code></pre>

<p>This task is responsible for moving or copying the reports from a certain
incoming AWS bucket to the private ooni-pipeline bucket ready for being
processed by the <code>generate_streams</code> task.</p>
<p>It will look inside of src for all files ending with <code>.yaml</code>.</p>
<p>The files will be renamed when moving them over to the dst directory using the
following format: <code>{date}-{asn}-{test_name}-{df_version}-{ext}</code>.</p>
<ul>
<li>
<p><strong>src</strong>: From from where the reports should be copied or moved. In the
ooni-pipeline this is set to <code>s3://ooni-incoming/</code> that is the incoming
bucket.</p>
</li>
<li>
<p><strong>dst</strong> default: <code>s3n://ooni-private/reports-raw/yaml/</code></p>
</li>
</ul>
<p>Where the reports should be moved or copied to.</p>
<ul>
<li><strong>workers</strong> default: 16</li>
</ul>
<p>The number of CPU workers to use when running the operations.</p>
<ul>
<li><strong>move</strong></li>
</ul>
<p>If the source file should be deleted once it has been successfully copied.</p>
<ul>
<li><strong>halt</strong>: default: disabled</li>
</ul>
<p>This is an optional flag that indicates if we should or should not halt the
machine when done.</p>
<h3 id="list-reports">List reports</h3>
<p>This task can be run via:</p>
<pre><code>invoke list_reports --path=URI
</code></pre>

<p>Will list all the files that appear to be OONI reports in a
certain directory.</p>
<ul>
<li><strong>path</strong>: default: <code>s3n://ooni-private/reports-raw/yaml/</code></li>
</ul>
<p>That path to list reports inside of.</p>
<h3 id="clean-streams">Clean streams</h3>
<p>This task can be run via:</p>
<pre><code>invoke clean_streams --dst-private=URI --dst-public=URI
</code></pre>

<p>This will delete all the files that are generated by the <code>generate_streams</code>
task.
In particular these files are:</p>
<ul>
<li>
<p><code>PRIVATE/reports-raw/streams</code></p>
</li>
<li>
<p><code>PRIVATE/reports-sanitised/yaml</code></p>
</li>
<li>
<p><code>PRIVATE/reports-sanitised/streams</code></p>
</li>
</ul>
<p>The arguments are:</p>
<ul>
<li><strong>dst_private</strong>: default: <code>s3n://ooni-private/</code></li>
</ul>
<p>The directory that contains the raw reports.</p>
<ul>
<li><strong>dst-public</strong>: default: <code>s3n://ooni-public/</code></li>
</ul>
<p>The directory that contains the public reports.</p>
<h3 id="add-headers-to-db">Add headers to DB</h3>
<p>This task can be run via:</p>
<pre><code>invoke add_headers_to_db --date-interval=DATE_INTERVAL [--src=URI]
                        [--workers=NUM --dst-private=URI --dst-public=URI]
                        [--halt]
</code></pre>

<p>When no date is specified it will run <code>upload_reports</code> on <code>s3n://ooni-incoming</code>
by moving them, then run on these incoming reports the add_headers_to_db batch
operation.
When a date range is specified it will run the batch operation on such date.</p>
<p>The batch operation will sanitise the YAML reports generating their streams and
then add the report headers to the database.
For the schema of the database see the avro specification inside of
<code>pipeline/helpers/report.py</code>.</p>
<ul>
<li><strong>date-interval</strong></li>
</ul>
<p>The date range that should be taken into consideration when running the
<code>generate_streams</code> task. If no date range is specified it will run against all
the dates.
The format for the date range is that of the <a href="http://luigi.readthedocs.org/en/stable/api/luigi.date_interval.html">luigi DateInterval
module</a>.
For example: <code>2019-10</code> will be the full month of October 2019 or
<code>2019-10-29-2019-10-31</code> will be the dates of the 29th and 30th of 2019.</p>
<ul>
<li><strong>src</strong> default: <code>s3n://ooni-private/reports-raw/yaml/</code></li>
</ul>
<p>Where the reports should be read from. This is considered the <strong>master
dataset</strong> of the pipeline.</p>
<ul>
<li><strong>workers</strong> default: 16</li>
</ul>
<p>The number of CPU workers to use when running the operations.</p>
<ul>
<li><em>dst_private</em>: default: <code>s3n://ooni-private/</code></li>
</ul>
<p>The target location in which to place the processed JSON streams. They will end
up inside of <code>$URI/reports-raw/streams/</code>.</p>
<ul>
<li><strong>dst_public</strong>: default: `s3n://ooni-public/"</li>
</ul>
<p>The target location in which to place the sanitised and processed JSON streams.
They will end up inside of <code>$URI/reports-sanitised/streams/</code>.</p>
<ul>
<li><strong>halt</strong>: default: disabled</li>
</ul>
<p>This is an optional flag that indicates if we should or should not halt the
machine when done.</p>
<h3 id="sync-reports">Sync reports</h3>
<p>This task can be run via:</p>
<pre><code>invoke sync_reports --srcs=URI [--dst-private=URI --workers=INT --halt]
</code></pre>

<p>This task is responsible for moving the reports from a certain set of sources
to the incoming bucket.
This task is usually run on collectors to place the data they have gathered
into the incoming AWS bucket.</p>
<p>It will look inside of src for all files ending with <code>.yaml</code>.</p>
<ul>
<li><strong>srcs</strong>: default: <code>ssh://root@bouncer.infra.ooni.nu/data/bouncer/archive</code></li>
</ul>
<p>The source directories from where to look for OONI reports.</p>
<ul>
<li><strong>dst-private</strong> default: <code>s3n://ooni-incoming/</code></li>
</ul>
<p>Where the reports should be moved to.</p>
<ul>
<li><strong>workers</strong> default: 16</li>
</ul>
<p>The number of CPU workers to use when running the operations.</p>
<ul>
<li><strong>halt</strong>: default: disabled</li>
</ul>
<p>This is an optional flag that indicates if we should or should not halt the
machine when done.</p>
<h3 id="start-computer">Start computer</h3>
<p>This task can be run via:</p>
<pre><code>invoke [--private-key=PATH instance-type=INSTANCE_TYPE --invoke_command=INVOKE_COMMAND]
</code></pre>

<ul>
<li><strong>private-key</strong>: default: <code>private/ooni-pipeline.pem</code></li>
</ul>
<p>The private key to be used to ssh into the machine.</p>
<ul>
<li><strong>instance_type</strong>: default: <code>c3.8xlarge</code></li>
</ul>
<p>The type of AWS EC2 instance to start. A full list of them can be found here:
<a href="https://aws.amazon.com/ec2/instance-types/">https://aws.amazon.com/ec2/instance-types/</a></p>
<ul>
<li><strong>invoke_command</strong>: default: <code>add_headers_to_db --workers=32 --halt</code></li>
</ul>
<p>The invoke command to be run once the machine is started. Remember that it may
be important to also run the --halt to avoid extra costs.</p>
<h3 id="spark-apps">Spark apps</h3>
<p>This task can be run via:</p>
<pre><code>invoke [--private-key=PATH instance-type=INSTANCE_TYPE --invoke_command=INVOKE_COMMAND]
</code></pre>

<p>This task will run the batch spark based apps on a hadoop cluster. The current
batch operations are responsible for inspecting the sanitised streams bucketed
by date located inside of <code>--src</code>, generating some database views based upon them
and writing a processed JSON file inside of <code>--dst</code> to indicate that the certain
date has been processed.</p>
<ul>
<li><strong>date-interval</strong></li>
</ul>
<p>The date range that should be taken into consideration when running the
<code>generate_streams</code> task. If no date range is specified it will run against all
the dates.
The format for the date range is that of the <a href="http://luigi.readthedocs.org/en/stable/api/luigi.date_interval.html">luigi DateInterval
module</a>.
For example: <code>2019-10</code> will be the full month of October 2019 or
<code>2019-10-29-2019-10-31</code> will be the dates of the 29th and 30th of 2019.</p>
<ul>
<li><strong>src</strong>: default: <code>s3n://ooni-public/reports-sanitised/streams/</code></li>
</ul>
<p>From where to read the JSON streams from.</p>
<ul>
<li><strong>dst</strong>: default: <code>s3n://ooni-public/processed/</code></li>
</ul>
<p>Where to write a file to indicate a certain date has been processed.</p>
<ul>
<li><strong>workers</strong>: default: 3</li>
</ul>
<p>The number of CPU workers to use when running the operations.</p>
<h3 id="spark-submit">Spark submit</h3>
<p>This task is work in progress and is not throughly tested, it's for running
spark scripts on a hadoop cluster.</p>
<h2 id="more-sauce">More sauce</h2>
<p>There is more, but the source is your friend, luke :)</p>
<p>Relevant information can be found inside of:</p>
<ul>
<li>
<p><code>invoke.yaml</code> - configuration file for invoke</p>
</li>
<li>
<p><code>tasks.py</code> - all the tasks run by <a href="http://www.pyinvoke.org/">invoke</a></p>
</li>
<li>
<p><code>playbook.yaml</code> - the ansible playbook used by invoke</p>
</li>
</ul>
<p>There is more to explore, but for the moment this is all folks.</p>
              
            </div>
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
	  
        </div>
      </div>

    </section>

  </div>

<div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
      
    </span>
</div>

</body>
</html>

<!--
MkDocs version : 0.14.0
Build Date UTC : 2015-11-25 21:33:26.440246
-->
